{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "import joblib\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admission number</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Follow_up time</th>\n",
       "      <th>Revasc</th>\n",
       "      <th>Culp</th>\n",
       "      <th>MainVASc</th>\n",
       "      <th>Numsten</th>\n",
       "      <th>ECG</th>\n",
       "      <th>Killip</th>\n",
       "      <th>...</th>\n",
       "      <th>UA</th>\n",
       "      <th>Glu</th>\n",
       "      <th>WBC</th>\n",
       "      <th>N</th>\n",
       "      <th>HyperP</th>\n",
       "      <th>Cigaret</th>\n",
       "      <th>OtherHis</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>Drugcom</th>\n",
       "      <th>MACE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>1038735</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>433</td>\n",
       "      <td>3260</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>455.0</td>\n",
       "      <td>4.13</td>\n",
       "      <td>9.28</td>\n",
       "      <td>7.06</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1000681</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>646</td>\n",
       "      <td>251</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>307.0</td>\n",
       "      <td>9.77</td>\n",
       "      <td>10.91</td>\n",
       "      <td>8.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1035609</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>581</td>\n",
       "      <td>270</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>406.0</td>\n",
       "      <td>5.01</td>\n",
       "      <td>4.60</td>\n",
       "      <td>2.75</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1027073</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>497</td>\n",
       "      <td>398</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>445.0</td>\n",
       "      <td>5.77</td>\n",
       "      <td>6.33</td>\n",
       "      <td>4.85</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>1050791</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>377</td>\n",
       "      <td>132</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>402.0</td>\n",
       "      <td>5.89</td>\n",
       "      <td>13.36</td>\n",
       "      <td>12.04</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     admission number  Age  Sex  Follow_up time  Revasc  Culp  MainVASc  \\\n",
       "342           1038735   49    0             433    3260     4         1   \n",
       "31            1000681   63    0             646     251     6         4   \n",
       "2             1035609   68    1             581     270     2         1   \n",
       "77            1027073   80    0             497     398     5         0   \n",
       "371           1050791   35    0             377     132     2         2   \n",
       "\n",
       "     Numsten  ECG  Killip  ...     UA   Glu    WBC      N  HyperP  Cigaret  \\\n",
       "342        1    1       1  ...  455.0  4.13   9.28   7.06       0        1   \n",
       "31         2    1       4  ...  307.0  9.77  10.91   8.12       1        1   \n",
       "2          1    1       3  ...  406.0  5.01   4.60   2.75       1        0   \n",
       "77         1    0       1  ...  445.0  5.77   6.33   4.85       0        0   \n",
       "371        3    1       1  ...  402.0  5.89  13.36  12.04       1        0   \n",
       "\n",
       "     OtherHis  diabetes  Drugcom  MACE  \n",
       "342         0         0        1     0  \n",
       "31          0         0        1     0  \n",
       "2           1         0        0     1  \n",
       "77          1         0        1     0  \n",
       "371         1         0        1     0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_excel('processed_train.xlsx',index_col=0)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(['admission number','MACE'],axis=1)\n",
    "y = train['MACE']\n",
    "\n",
    "X=(X-np.min(X))/(np.max(X)-np.min(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_leaf': 6, 'min_samples_split': 2}\n",
      "0.6640106951871658\n"
     ]
    }
   ],
   "source": [
    "#hyperparameter tune of DT\n",
    "DT_model = DecisionTreeClassifier(class_weight='balanced',random_state=1,max_depth=6,\n",
    "                                 max_features=5,criterion='gini',splitter='best')\n",
    "'''\n",
    "parameters = {'max_depth':range(2,20),'max_features':['log2','sqrt',1,2,3,4,5,6,7,8,9,10,11,12,13,14],\n",
    "              'criterion':['gini','entropy'],'splitter':['best','random'],}\n",
    "step1:parameter1 = {'max_depth':range(2,20)}，best_params_：{'max_depth': 6}\n",
    "step2:parameter2 = {'max_features':['log2','sqrt',1,2,3,4,5,6,7,8,9,10,11,12,13,14]}，best_params_：{{'max_features': 5}}\n",
    "step3:parameter3 = {'criterion':['gini','entropy']}，best_params_：{'criterion': 'gini'}\n",
    "step4:parameter4 = {'splitter':['best','random']}，best_params_：{'splitter': 'best'}\n",
    "step5:parameter5 = {'min_samples_leaf':range(1,10),'min_samples_split':range(2,10)}，\n",
    "best_params_：{'min_samples_leaf': 6, 'min_samples_split': 2}\n",
    "\n",
    "Thus, the best model: DecisionTreeClassifier(class_weight='balanced',random_state=1,\n",
    "                                  max_depth=6,max_features=5,criterion='gini',\n",
    "                                 splitter='best',min_samples_leaf=6,min_samples_split=2)\n",
    "'''\n",
    "parameter5 = {'min_samples_leaf':range(1,10),'min_samples_split':range(2,10)}\n",
    "GS = GridSearchCV(DT_model, parameter5,scoring='roc_auc', cv=5)\n",
    "model = GS.fit(X,y)\n",
    "\n",
    "print(model.best_params_)\n",
    "'''\n",
    "tuned_DT_model =  DecisionTreeClassifier(class_weight='balanced',random_state=1,\n",
    "                                  max_depth=6,max_features=5,criterion='gini',\n",
    "                                 splitter='best',min_samples_leaf=6,min_samples_split=2)\n",
    "'''\n",
    "tuned_DT_model = model.best_estimator_\n",
    "joblib.dump(tuned_DT_model,filename='best_DT_model')\n",
    "print(model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['calibrate_DT']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calibration for decision tree\n",
    "isotonic = CalibratedClassifierCV(model.best_estimator_, cv=5, method='isotonic')\n",
    "calibrate_DT = isotonic.fit(X,y)\n",
    "joblib.dump(calibrate_DT,filename='calibrate_DT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: [0.57941176 0.70686275 0.64411765 0.5745098  0.81515152]\n",
      "Mean AUC: 0.6640106951871658\n",
      "95% CI of AUC (0.4881745767221536, 0.8398468136521781)\n",
      "Acc [0.57142857 0.69387755 0.67346939 0.51020408 0.77083333]\n",
      "Mean Acc 0.6439625850340136\n",
      "95% CI of AUC (0.46302730759099175, 0.8248978624770354)\n",
      "F1_score [0.46153846 0.59459459 0.52941176 0.42857143 0.64516129]\n",
      "Mean F1 0.5318555079465894\n",
      "95% CI of F1 (0.3740998263955928, 0.689611189497586)\n"
     ]
    }
   ],
   "source": [
    "#caculate AUC in the internal validation cohort\n",
    "parameter = {'min_samples_leaf':[6],'min_samples_split':[2]}\n",
    "GS1 = GridSearchCV(DT_model, parameter,scoring='roc_auc', cv=5)\n",
    "model1 = GS1.fit(X,y)\n",
    "result = model1.cv_results_\n",
    "auc_result = np.concatenate([result['split0_test_score'],result['split1_test_score'],\n",
    "               result['split2_test_score'],result['split3_test_score'],\n",
    "               result['split4_test_score']],axis=0)\n",
    "#caculate 95% CI of AUC\n",
    "interval1=stats.norm.interval(0.95,np.mean(auc_result),np.std(auc_result))\n",
    "print('AUC:',auc_result)\n",
    "print('Mean AUC:',np.mean(auc_result))\n",
    "print('95% CI of AUC',interval1)\n",
    "\n",
    "#caculate accuracy in the internal validation cohort\n",
    "GS2 = GridSearchCV(DT_model, parameter,scoring='accuracy', cv=5)\n",
    "model2 = GS2.fit(X,y)\n",
    "result = model2.cv_results_\n",
    "acc_result = np.concatenate([result['split0_test_score'],result['split1_test_score'],\n",
    "               result['split2_test_score'],result['split3_test_score'],\n",
    "               result['split4_test_score']],axis=0)\n",
    "interval2=stats.norm.interval(0.95,np.mean(acc_result),np.std(acc_result))\n",
    "print('Acc',acc_result)\n",
    "print('Mean Acc',np.mean(acc_result))\n",
    "print('95% CI of AUC',interval2)\n",
    "\n",
    "#caculate accuracy in the internal validation cohort\n",
    "GS3 = GridSearchCV(DT_model, parameter,scoring='f1', cv=5)\n",
    "model3 = GS3.fit(X,y)\n",
    "result = model3.cv_results_\n",
    "f1_result = np.concatenate([result['split0_test_score'],result['split1_test_score'],\n",
    "               result['split2_test_score'],result['split3_test_score'],\n",
    "               result['split4_test_score']],axis=0)\n",
    "interval3=stats.norm.interval(0.95,np.mean(f1_result),np.std(f1_result))\n",
    "print('F1_score',f1_result)\n",
    "print('Mean F1',np.mean(f1_result))\n",
    "print('95% CI of F1',interval3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_intercept': True, 'intercept_scaling': 0.5}\n",
      "0.7172727272727274\n"
     ]
    }
   ],
   "source": [
    "#hyperparameter tune of LR\n",
    "LR_model = LogisticRegression(class_weight=\"balanced\",penalty='l2',random_state=1,\n",
    "                              dual=False,C=1,solver='lbfgs')\n",
    "'''\n",
    "parameters ={'penalty':['l1','l2'],'dual'[True,False],'solver':['liblinear','lbfgs','newton-cg','sag','saga'],\n",
    "'C':[0.01,0.05,0.1,0.5,1,5,10,50],'fit_intercept':[True,False],'intercept_scaling':[-0.5,0.5,1,1.5,2]}\n",
    "\n",
    "if penalty = l1\n",
    "step1:parameter1 = {'penalty':['l1'],'dual':[False],'solver':['liblinear','saga']} \n",
    "best_params_:{'dual': False, 'penalty': 'l1', 'solver': 'liblinear'}\n",
    "step2:parameter2 = {'C':[0.01,0.05,0.1,0.5,1,5,10,50]} best_params_: {'C': 5} best_score_:0.7127\n",
    "step3:parameter3 = {'fit_intercept':[True],'intercept_scaling':[0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.9,0.95,1]} \n",
    "best_params_ = {'fit_intercept': True, 'intercept_scaling': 0.75} best_score_:0.7131\n",
    "when penalty = l1,tuned_LR_model =  LogisticRegression(class_weight=\"balanced\",random_state=1,penalty='l1',dual=False,\n",
    "                                    solver='liblinear',C=5,fit_intercept=True,intercept_scaling=0.75)\n",
    "\n",
    "if penalty = l2,dual = True,solver = liblinear\n",
    "step1:parameter1 = {'penalty':['l2],'dual':[True],'solver':['liblinear'],'C':[0.01,0.05,0.1,0.5,1,5,10,50]}\n",
    "best_params_: {'C': 1, 'dual': True, 'penalty': 'l2', 'solver': 'liblinear'} best_score_:0.7168\n",
    "step2:parameter2 = {'fit_intercept':[True],'intercept_scaling':[0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.9,0.95,1]} \n",
    "best_params_:{'fit_intercept': True, 'intercept_scaling': 0.9} best_score_:0.7168\n",
    "Thus,tuned_LR_model = LogisticRegression(class_weight=\"balanced\",penalty='l2',random_state=1,\n",
    "                              dual=True,solver='liblinear',C=1,fit_intercept=False)\n",
    "\n",
    "if penalty = l2,dual = False\n",
    "step1:parameter1 = {'penalty':['l2],'dual':[False],'solver':['liblinear','lbfgs','newton-cg','saga']}\n",
    "best_params_:{'C': 1, 'dual': False, 'penalty': 'l2', 'solver': 'lbfgs'} best_score_:0.7173\n",
    "step2:parameter2 = {'fit_intercept':[True],'intercept_scaling':[0.5,0.75,1,1.25,1.5]} \n",
    "best_params_:{'fit_intercept': True, 'intercept_scaling': 0.5}  best_score_:0.7173 \n",
    "Thus,the best tuned LR model is:best_LR_model = LogisticRegression(class_weight=\"balanced\",penalty='l2',random_state=1,\n",
    "                              dual=False,C=1,solver='lbfgs',fit_intercept=True,intercept_scaling=0.5)\n",
    "                              \n",
    "Above all,the best tuned LR model is:LogisticRegression(class_weight=\"balanced\",penalty='l2',random_state=1,\n",
    "                                                        dual=False,C=1,solver='lbfgs',fit_intercept=True,intercept_scaling=0.5\n",
    "'''\n",
    "parameter2 = {'fit_intercept':[True],'intercept_scaling':[0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.9,0.95,1]}\n",
    "GS = GridSearchCV(LR_model, parameter2,scoring='roc_auc', cv=5)\n",
    "model = GS.fit(X,y)\n",
    "'''\n",
    "tuned_LR_model = LogisticRegression(class_weight=\"balanced\",penalty='l2',random_state=1,\n",
    "                              solver='liblinear',C=0.25,fit_intercept=True,intercept_scaling=1)\n",
    "'''\n",
    "tuned_LR_model = model.best_estimator_\n",
    "joblib.dump(tuned_LR_model,filename='best_LR_model')\n",
    "print(model.best_params_)\n",
    "print(model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['calibrate_LR']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calibration for logistic regression \n",
    "isotonic = CalibratedClassifierCV(model.best_estimator_, cv=5, method='isotonic')\n",
    "calibrate_LR = isotonic.fit(X,y)\n",
    "joblib.dump(calibrate_LR,filename='calibrate_LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: [0.70196078 0.73529412 0.7254902  0.72058824 0.7030303 ]\n",
      "Mean AUC: 0.7172727272727274\n",
      "95% CI of AUC (0.6918597380572815, 0.7426857164881733)\n",
      "Acc [0.69387755 0.73469388 0.7755102  0.73469388 0.66666667]\n",
      "Mean Acc 0.7210884353741496\n",
      "95% CI of AUC (0.6475748595503642, 0.7946020111979349)\n",
      "F1_score [0.51612903 0.60606061 0.62068966 0.55172414 0.52941176]\n",
      "Mean F1 0.5648030392256002\n",
      "95% CI of F1 (0.4834314356827193, 0.646174642768481)\n"
     ]
    }
   ],
   "source": [
    "#LR\n",
    "#caculate AUC in the internal validation cohort\n",
    "parameter = {'fit_intercept':[True],'intercept_scaling':[0.5]}\n",
    "GS1 = GridSearchCV(LR_model, parameter,scoring='roc_auc', cv=5)\n",
    "model1 = GS1.fit(X,y)\n",
    "result = model1.cv_results_\n",
    "auc_result = np.concatenate([result['split0_test_score'],result['split1_test_score'],\n",
    "               result['split2_test_score'],result['split3_test_score'],\n",
    "               result['split4_test_score']],axis=0)\n",
    "#caculate 95% CI of AUC\n",
    "interval1=stats.norm.interval(0.95,np.mean(auc_result),np.std(auc_result))\n",
    "print('AUC:',auc_result)\n",
    "print('Mean AUC:',np.mean(auc_result))\n",
    "print('95% CI of AUC',interval1)\n",
    "\n",
    "#caculate accuracy in the internal validation cohort\n",
    "GS2 = GridSearchCV(LR_model, parameter,scoring='accuracy', cv=5)\n",
    "model2 = GS2.fit(X,y)\n",
    "result = model2.cv_results_\n",
    "acc_result = np.concatenate([result['split0_test_score'],result['split1_test_score'],\n",
    "               result['split2_test_score'],result['split3_test_score'],\n",
    "               result['split4_test_score']],axis=0)\n",
    "interval2=stats.norm.interval(0.95,np.mean(acc_result),np.std(acc_result))\n",
    "print('Acc',acc_result)\n",
    "print('Mean Acc',np.mean(acc_result))\n",
    "print('95% CI of AUC',interval2)\n",
    "\n",
    "#caculate accuracy in the internal validation cohort\n",
    "GS3 = GridSearchCV(LR_model, parameter,scoring='f1', cv=5)\n",
    "model3 = GS3.fit(X,y)\n",
    "result = model3.cv_results_\n",
    "f1_result = np.concatenate([result['split0_test_score'],result['split1_test_score'],\n",
    "               result['split2_test_score'],result['split3_test_score'],\n",
    "               result['split4_test_score']],axis=0)\n",
    "interval3=stats.norm.interval(0.95,np.mean(f1_result),np.std(f1_result))\n",
    "print('F1_score',f1_result)\n",
    "print('Mean F1',np.mean(f1_result))\n",
    "print('95% CI of F1',interval3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_NB_model']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GaussianNB has no parameters that can be optimized, therefore we just perform 5_fold crossvalidation,\n",
    "# then caculate the AUC in each split validation cohort\n",
    "X1 = X.copy()\n",
    "X1.index = range(len(X))\n",
    "y1 = y.copy()\n",
    "y1.index = range(len(y))\n",
    "\n",
    "k = 5\n",
    "num_val_samples = len(X1) // k\n",
    "validation_result = []\n",
    "Auc = []\n",
    "Acc = []\n",
    "F1 = []\n",
    "#5_Fold cross_validation\n",
    "for i in range(k):\n",
    "    val_data = X1[i * num_val_samples: (i+1) * num_val_samples]\n",
    "    val_targrt = y1[i * num_val_samples: (i+1) * num_val_samples]\n",
    "    \n",
    "    train_data = np.concatenate([X1[:i * num_val_samples],\n",
    "                               X1[(i+1) * num_val_samples:]],axis = 0)\n",
    "    train_targrt = np.concatenate([y1[:i * num_val_samples],\n",
    "                                   y1[(i+1) * num_val_samples:]],axis = 0)\n",
    "    NB_model = GaussianNB()\n",
    "    NB_model = NB_model.fit(train_data,train_targrt)\n",
    "    y_proba = NB_model.predict_proba(val_data)\n",
    "    y_pred = NB_model.predict(val_data)\n",
    "\n",
    "    Auc.append(metrics.roc_auc_score(val_targrt,y_proba[:,1]))\n",
    "    Acc.append(metrics.accuracy_score(val_targrt,y_pred))\n",
    "    F1.append(metrics.f1_score(val_targrt,y_pred))\n",
    "tuned_NB_model = NB_model\n",
    "joblib.dump(tuned_NB_model,filename='best_NB_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['calibrate_NB']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calibration for Naive Bayes\n",
    "isotonic = CalibratedClassifierCV(GaussianNB(), cv=5, method='isotonic')\n",
    "calibrate_NB = isotonic.fit(X,y)\n",
    "joblib.dump(calibrate_NB,filename='calibrate_NB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: [0.73109243697479, 0.6875, 0.6851851851851851, 0.7849364791288566, 0.7777777777777778]\n",
      "Mean AUC: 0.7332983758133219\n",
      "95% CI of AUC (0.6498575391245583, 0.8167392125020855)\n",
      "Acc [0.7083333333333334, 0.7083333333333334, 0.75, 0.75, 0.7916666666666666]\n",
      "Mean Acc 0.7416666666666667\n",
      "95% CI of AUC (0.6805540523286235, 0.8027792810047099)\n",
      "F1_score [0.4615384615384615, 0.4166666666666667, 0.45454545454545453, 0.6, 0.5833333333333334]\n",
      "Mean F1 0.5032167832167833\n",
      "95% CI of F1 (0.3581726785360275, 0.6482608878975391)\n"
     ]
    }
   ],
   "source": [
    "#NB\n",
    "#caculate AUC in the internal validation cohort\n",
    "#caculate 95% CI of AUC\n",
    "interval1=stats.norm.interval(0.95,np.mean(Auc),np.std(Auc))\n",
    "print('AUC:',Auc)\n",
    "print('Mean AUC:',np.mean(Auc))\n",
    "print('95% CI of AUC',interval1)\n",
    "\n",
    "#caculate accuracy in the internal validation cohort\n",
    "interval2=stats.norm.interval(0.95,np.mean(Acc),np.std(Acc))\n",
    "print('Acc',Acc)\n",
    "print('Mean Acc',np.mean(Acc))\n",
    "print('95% CI of AUC',interval2)\n",
    "\n",
    "#caculate accuracy in the internal validation cohort\n",
    "interval3=stats.norm.interval(0.95,np.mean(F1),np.std(F1))\n",
    "print('F1_score',F1)\n",
    "print('Mean F1',np.mean(F1))\n",
    "print('95% CI of F1',interval3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 'squared_hinge'}\n",
      "0.7133986928104574\n"
     ]
    }
   ],
   "source": [
    "Linear_SVM_model = LinearSVC(class_weight='balanced',random_state=1,penalty='l1',dual=False,\n",
    "                            fit_intercept=False,C=0.5)\n",
    "'''\n",
    "parameters = {'penalty':['l2','l1'],'dual':[True,False],'loss':['squared_hinge','hinge'],'fit_intercept':[True,False],\n",
    "             'C':[0.001,0.005,0.1,0.5,1,5,10],'intercept_scaling':[-0.5,0.5,1,1.5,2]}\n",
    "penalty = l2\n",
    "step1:parameter1 = {'penalty':['l2'],'dual':[True,False]} best_params_:{'dual': True, 'penalty': 'l2'}\n",
    "step2:parameter2 = {'fit_intercept':[True,False]} best_params_: {'fit_intercept': True} best_score_:0.7099\n",
    "step3:parameter3 = {'fit_intercept':[True],'intercept_scaling':[0.5,1,1.5,2]} best_params_:{'fit_intercept': True, 'intercept_scaling': 1}\n",
    "best_score_:0.7084. Thus,best_params_:{'fit_intercept': False}\n",
    "step4:parameter4 = {'C':[0.001,0.005,0.01,0.05,0.1,0.5,1,5,10]} best_params_:{'C': 0.1} best_score_:0.7168\n",
    "step5:parameter5 = {'loss':['squared_hinge','hinge']}  best_params_: {'loss': 'squared_hinge'} best_score_:0.7168\n",
    "if penalty = l2, Best model =  LinearSVC(class_weight='balanced',random_state=1,penalty = 'l2',dual=True,\n",
    "                            fit_intercept=False,C=0.1), best_score_:0.7168\n",
    "                            \n",
    "penalty = l1\n",
    "step1:parameter1 = {'penalty':['l1'],'dual':[False]} best_params_:{'dual': False, 'penalty': 'l1'} best_score_:0.709\n",
    "step2:parameter2 = {'fit_intercept':[True,False]} best_params_: {'fit_intercept': False} best_score_:0.7119\n",
    "step3:parameter3 = {'C':[0.001,0.005,0.01,0.05,0.1,0.5,1,5,10]}   best_params_:{'C': 0.5} best_score_:0.7133\n",
    "step4:parameter4 = {'loss':['squared_hinge','hinge']} best_params_:{'loss': 'squared_hinge'},best_score_:0.7288\n",
    "\n",
    "Above all, Best_LinearSVC_Model = LinearSVC(class_weight='balanced',random_state=1,penalty = 'l2',dual=True,\n",
    "                                  fit_intercept=False,C=0.1)\n",
    "'''\n",
    "parameter5 = {'loss':['squared_hinge']}\n",
    "GS = GridSearchCV(Linear_SVM_model, parameter5,scoring='roc_auc', cv=5)\n",
    "model = GS.fit(X,y)\n",
    "'''\n",
    "Best_LinearSVC_Model = LinearSVC(class_weight='balanced',random_state=1,penalty = 'l2',dual=True,\n",
    "                                 fit_intercept=False,C=0.025,loss='squared_hinge')\n",
    "'''\n",
    "Tuned_LinearSVC_Model = model.best_estimator_\n",
    "joblib.dump(Tuned_LinearSVC_Model,filename='Best_LinearSVC_Model')\n",
    "print(model.best_params_)\n",
    "print(model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['calibrate_LSVC']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calibration for LinearSVC\n",
    "isotonic = CalibratedClassifierCV(model.best_estimator_, cv=5, method='isotonic')\n",
    "calibrate_LSVC = isotonic.fit(X,y)\n",
    "joblib.dump(calibrate_LSVC,filename='calibrate_LSVC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: [0.70196078 0.7372549  0.72352941 0.7245098  0.6969697 ]\n",
      "Mean AUC: 0.7168449197860963\n",
      "95% CI of AUC (0.6872947692473245, 0.7463950703248682)\n",
      "Acc [0.69387755 0.71428571 0.7755102  0.7755102  0.66666667]\n",
      "Mean Acc 0.7251700680272108\n",
      "95% CI of Acc (0.6393397681977937, 0.811000367856628)\n",
      "F1_score [0.51612903 0.5625     0.62068966 0.62068966 0.52941176]\n",
      "Mean F1 0.5698840214617549\n",
      "95% CI of F1 (0.48335854366796327, 0.6564094992555465)\n"
     ]
    }
   ],
   "source": [
    "#LinearSVC\n",
    "#caculate AUC in the internal validation cohort\n",
    "parameter = {'loss':['squared_hinge']}\n",
    "Linear_SVM_model = LinearSVC(class_weight='balanced',random_state=1,penalty = 'l2',dual=True,\n",
    "                            fit_intercept=False,C=0.1)\n",
    "GS1 = GridSearchCV(Linear_SVM_model, parameter,scoring='roc_auc', cv=5)\n",
    "model1 = GS1.fit(X,y)\n",
    "result = model1.cv_results_\n",
    "auc_result = np.concatenate([result['split0_test_score'],result['split1_test_score'],\n",
    "               result['split2_test_score'],result['split3_test_score'],\n",
    "               result['split4_test_score']],axis=0)\n",
    "#caculate 95% CI of AUC\n",
    "interval1=stats.norm.interval(0.95,np.mean(auc_result),np.std(auc_result))\n",
    "print('AUC:',auc_result)\n",
    "print('Mean AUC:',np.mean(auc_result))\n",
    "print('95% CI of AUC',interval1)\n",
    "\n",
    "#caculate accuracy in the internal validation cohort\n",
    "GS2 = GridSearchCV(Linear_SVM_model, parameter,scoring='accuracy', cv=5)\n",
    "model2 = GS2.fit(X,y)\n",
    "result = model2.cv_results_\n",
    "acc_result = np.concatenate([result['split0_test_score'],result['split1_test_score'],\n",
    "               result['split2_test_score'],result['split3_test_score'],\n",
    "               result['split4_test_score']],axis=0)\n",
    "interval2=stats.norm.interval(0.95,np.mean(acc_result),np.std(acc_result))\n",
    "print('Acc',acc_result)\n",
    "print('Mean Acc',np.mean(acc_result))\n",
    "print('95% CI of Acc',interval2)\n",
    "\n",
    "#caculate accuracy in the internal validation cohort\n",
    "GS3 = GridSearchCV(Linear_SVM_model, parameter,scoring='f1', cv=5)\n",
    "model3 = GS3.fit(X,y)\n",
    "result = model3.cv_results_\n",
    "f1_result = np.concatenate([result['split0_test_score'],result['split1_test_score'],\n",
    "               result['split2_test_score'],result['split3_test_score'],\n",
    "               result['split4_test_score']],axis=0)\n",
    "interval3=stats.norm.interval(0.95,np.mean(f1_result),np.std(f1_result))\n",
    "print('F1_score',f1_result)\n",
    "print('Mean F1',np.mean(f1_result))\n",
    "print('95% CI of F1',interval3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 'auto'}\n",
      "0.7076232917409389\n"
     ]
    }
   ],
   "source": [
    "SVM_model = SVC(class_weight='balanced',probability=True,random_state=1,kernel='linear',\n",
    "               C=2.5)\n",
    "'''\n",
    "parameters = {'kernel':['rbf','poly','linear','sigmod','precomputed'],'degree':[2,3,4,5,6]\n",
    ",'C':[0.01,0.1,1,5,10],'gamma':[0.01,0.03, 0.05,0.07,0.09, 0.1, 0.5],'coef0':[0.0]}\n",
    "step1:parameter1 = {'kernel':['rbf','poly','linear','sigmod','precomputed']}，best_params_：{'kernel': 'linear'}\n",
    "step2:parameter2 = {'C':[0.01,0.05,0.1,0.5,1]}，best_params_：{'C': 2.5}\n",
    "step3:parameter3 = {'gamma':['auto',0.001,0.005,0.01,0.05,0.1]}，best_params_：{'gamma': 'auto'} best_score_:0.7076\n",
    "\n",
    "Thus,we choose LinearSVC\n",
    "'''\n",
    "parameter3 = {'gamma':['auto',0.001,0.005,0.01,0.05,0.1]}\n",
    "GS = GridSearchCV(SVM_model, parameter3,scoring='roc_auc', cv=5)\n",
    "model = GS.fit(X,y)\n",
    "'''\n",
    "Best_SVC_Model = SVC(class_weight='balanced',probability=True,random_state=1,\n",
    "               kernel='linear',C=0.05,gamma='auto')\n",
    "'''\n",
    "Tuned_SVC_Model = model.best_estimator_\n",
    "joblib.dump(Tuned_SVC_Model,filename='Best_SVC_Model')\n",
    "print(model.best_params_)\n",
    "print(model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7076232917409389\n",
      "0.7076232917409389\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.67843137, 0.70980392, 0.70980392, 0.71078431, 0.72929293])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.cv_results_\n",
    "a = len(result['split0_test_score'])\n",
    "auc_result = np.concatenate([result['split0_test_score'].reshape(a,1),result['split1_test_score'].reshape(a,1),\n",
    "               result['split2_test_score'].reshape(a,1),result['split3_test_score'].reshape(a,1),\n",
    "               result['split4_test_score'].reshape(a,1)],axis=1)\n",
    "SVM_validation_auc = auc_result[0,]\n",
    "#if model.best_score_ = np.mean(LR_validation_auc),we obtain the correct AUC in the validation cohort\n",
    "print(model.best_score_)\n",
    "print(np.mean(SVM_validation_auc))\n",
    "SVM_validation_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 3}\n",
      "0.748871063576946\n"
     ]
    }
   ],
   "source": [
    "RDF_model = RandomForestClassifier(class_weight=\"balanced\",random_state=1,n_estimators=50,\n",
    "                                  criterion='gini',max_depth=12,min_samples_leaf=5,\n",
    "                                  min_samples_split=2)\n",
    "'''\n",
    "parameters = {'criterion':['gini','entropy'],'n_estimators':range(1,100),'max_depth':range(1,100),\n",
    ",'min_samples_split':range(2,10),'min_samples_leaf':range(1,10),'max_features':['sqrt',3,4,5,6,7]}\n",
    "step1:parameter1 = {'n_estimators':range(1,100)} best_params_:{'n_estimators':50}\n",
    "step2:parameter2 = {'criterion':['gini','entropy']} best_params_:{'criterion': 'gini'}\n",
    "step3:parameter3 = {'max_depth':range(1,100)} best_params_:{'max_depth': 12}\n",
    "step4:parameter4 = {'min_samples_split':range(2,10),'min_samples_leaf':range(1,10)} best_params_:{'min_samples_leaf': 5, 'min_samples_split': 2}\n",
    "step5:parameter5 = {'max_features':['sqrt','log2',2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]} \n",
    "best_params_:{'max_features': 3}\n",
    "Thus, the best RDF model:Best_RDF_Model =  RandomForestClassifier(class_weight=\"balanced\",random_state=1,n_estimators=70\n",
    "                                  ,criterion='gini',max_depth=5,min_samples_split=2,\n",
    "                                  min_samples_leaf=5,max_features=3)\n",
    "\n",
    "'''\n",
    "parameter5 = {'max_features':['sqrt','log2',2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]} \n",
    "GS = GridSearchCV(RDF_model, parameter5,scoring='roc_auc', cv=5)\n",
    "model = GS.fit(X,y)\n",
    "'''\n",
    "Best_RDF_Model = RandomForestClassifier(class_weight=\"balanced\",random_state=1,n_estimators=70\n",
    "                                        ,criterion='gini',max_depth=5,min_samples_split=2,\n",
    "                                        min_samples_leaf=5,max_features='sqrt')\n",
    "'''\n",
    "Tuned_RDF_Model = model.best_estimator_\n",
    "joblib.dump(Tuned_RDF_Model,filename='Best_RDF_Model')                                \n",
    "print(model.best_params_)\n",
    "print(model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['calibrate_RDF']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calibration for Random forest\n",
    "isotonic = CalibratedClassifierCV(model.best_estimator_, cv=5, method='isotonic')\n",
    "calibrate_RDF = isotonic.fit(X,y)\n",
    "joblib.dump(calibrate_RDF,filename='calibrate_RDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: [0.70196078 0.81764706 0.69215686 0.7245098  0.80808081]\n",
      "Mean AUC: 0.748871063576946\n",
      "95% CI of AUC (0.6442505535790908, 0.8534915735748012)\n",
      "Acc [0.69387755 0.79591837 0.75510204 0.67346939 0.75      ]\n",
      "Mean Acc 0.7336734693877551\n",
      "95% CI of Acc (0.6468649268875631, 0.8204820118879471)\n",
      "F1_score [0.44444444 0.5        0.5        0.38461538 0.57142857]\n",
      "Mean F1 0.48009768009768006\n",
      "95% CI of F1 (0.3576303272929534, 0.6025650329024067)\n"
     ]
    }
   ],
   "source": [
    "#RDF\n",
    "#caculate AUC in the internal validation cohort\n",
    "parameter = {'max_features':[3]}\n",
    "GS1 = GridSearchCV(RDF_model, parameter,scoring='roc_auc', cv=5)\n",
    "model1 = GS1.fit(X,y)\n",
    "result = model1.cv_results_\n",
    "auc_result = np.concatenate([result['split0_test_score'],result['split1_test_score'],\n",
    "               result['split2_test_score'],result['split3_test_score'],\n",
    "               result['split4_test_score']],axis=0)\n",
    "#caculate 95% CI of AUC\n",
    "interval1=stats.norm.interval(0.95,np.mean(auc_result),np.std(auc_result))\n",
    "print('AUC:',auc_result)\n",
    "print('Mean AUC:',np.mean(auc_result))\n",
    "print('95% CI of AUC',interval1)\n",
    "\n",
    "#caculate accuracy in the internal validation cohort\n",
    "GS2 = GridSearchCV(RDF_model, parameter,scoring='accuracy', cv=5)\n",
    "model2 = GS2.fit(X,y)\n",
    "result = model2.cv_results_\n",
    "acc_result = np.concatenate([result['split0_test_score'],result['split1_test_score'],\n",
    "               result['split2_test_score'],result['split3_test_score'],\n",
    "               result['split4_test_score']],axis=0)\n",
    "interval2=stats.norm.interval(0.95,np.mean(acc_result),np.std(acc_result))\n",
    "print('Acc',acc_result)\n",
    "print('Mean Acc',np.mean(acc_result))\n",
    "print('95% CI of Acc',interval2)\n",
    "\n",
    "#caculate accuracy in the internal validation cohort\n",
    "GS3 = GridSearchCV(RDF_model, parameter,scoring='f1', cv=5)\n",
    "model3 = GS3.fit(X,y)\n",
    "result = model3.cv_results_\n",
    "f1_result = np.concatenate([result['split0_test_score'],result['split1_test_score'],\n",
    "               result['split2_test_score'],result['split3_test_score'],\n",
    "               result['split4_test_score']],axis=0)\n",
    "interval3=stats.norm.interval(0.95,np.mean(f1_result),np.std(f1_result))\n",
    "print('F1_score',f1_result)\n",
    "print('Mean F1',np.mean(f1_result))\n",
    "print('95% CI of F1',interval3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subsample': 1}\n",
      "0.7374866310160428\n"
     ]
    }
   ],
   "source": [
    "GDB_model = GradientBoostingClassifier(random_state=1,n_estimators=31,min_samples_leaf=7,\n",
    "                                      min_samples_split=2,max_features='sqrt',max_depth=13)\n",
    "'''\n",
    "parameters ={'learning_rate':[0.001,0.005,0.01,0.05,0.1,0.5,1]\n",
    "                ,'n_estimators':range(1,100)\n",
    "                ,'max_depth':range(1,100),'min_samples_split':[2,3,4,5,6,7,8,9,10]\n",
    "                ,'max_features':[3,4,5,6,7],\n",
    "                ,'subsample':[0.6,0.7,0.75,0.8,0.85,0.9,0.95,1]\n",
    "                ,'criterion':['friedman_mse', 'mse', 'mae']\n",
    "                }\n",
    "step1:parameter1 = {'n_estimators':range(1,100)} best_params_:{'n_estimators': 31}\n",
    "step2:parameter2 = {'learning_rate':[0.001,0.005,0.01,0.05,0.1,0.5,1]} best_params_:{'learning_rate': 0.1}\n",
    "step3:parameter3 = {'min_samples_split':range(2,10),'min_samples_leaf':range(1,10)} best_params_:{'min_samples_leaf': 7, 'min_samples_split': 2}\n",
    "step4:parameter4 = {'max_features':['sqrt','log2',2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]} best_params_:{'max_features': 'sqrt'}\n",
    "step5:parameter5 = {'max_depth':range(1,100)} best_params_:{'max_depth': 13}\n",
    "step6:parameter6 = {'subsample':[0.6,0.7,0.75,0.8,0.85,0.9,0.95,1]} best_params_:{'subsample': 1}\n",
    "'''\n",
    "parameters = {'subsample':[0.6,0.7,0.75,0.8,0.85,0.9,0.95,1]} \n",
    "GS = GridSearchCV(GDB_model, parameters,scoring='roc_auc', cv=5)\n",
    "model = GS.fit(X,y)\n",
    "'''\n",
    "Best_GDB_Model =  GradientBoostingClassifier(random_state=1,n_estimators=30,learning_rate=0.1,\n",
    "                                      max_features=3,max_depth=2,min_samples_split=3,\n",
    "                                      subsample=0.7)\n",
    "'''\n",
    "Tuned_GDB_Model = model.best_estimator_\n",
    "joblib.dump(Tuned_GDB_Model,filename='Best_GDB_Model') \n",
    "print(model.best_params_)\n",
    "print(model.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['calibrate_GDB']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calibration for Random forest\n",
    "isotonic = CalibratedClassifierCV(model.best_estimator_, cv=5, method='isotonic')\n",
    "calibrate_GDB = isotonic.fit(X,y)\n",
    "joblib.dump(calibrate_GDB,filename='calibrate_GDB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: [0.65098039 0.8        0.72156863 0.7754902  0.73939394]\n",
      "Mean AUC: 0.7374866310160428\n",
      "95% CI of AUC (0.6371926012433831, 0.8377806607887026)\n",
      "Acc [0.67346939 0.71428571 0.75510204 0.81632653 0.6875    ]\n",
      "Mean Acc 0.7293367346938775\n",
      "95% CI of Acc (0.6281407681640724, 0.8305327012236826)\n",
      "F1_score [0.33333333 0.41666667 0.33333333 0.66666667 0.51612903]\n",
      "Mean F1 0.4532258064516129\n",
      "95% CI of F1 (0.20593023311449316, 0.7005213797887326)\n"
     ]
    }
   ],
   "source": [
    "#GDB\n",
    "#caculate AUC in the internal validation cohort\n",
    "parameter = {'subsample':[1]}\n",
    "GS1 = GridSearchCV(GDB_model, parameter,scoring='roc_auc', cv=5)\n",
    "model1 = GS1.fit(X,y)\n",
    "result = model1.cv_results_\n",
    "auc_result = np.concatenate([result['split0_test_score'],result['split1_test_score'],\n",
    "               result['split2_test_score'],result['split3_test_score'],\n",
    "               result['split4_test_score']],axis=0)\n",
    "#caculate 95% CI of AUC\n",
    "interval1=stats.norm.interval(0.95,np.mean(auc_result),np.std(auc_result))\n",
    "print('AUC:',auc_result)\n",
    "print('Mean AUC:',np.mean(auc_result))\n",
    "print('95% CI of AUC',interval1)\n",
    "\n",
    "#caculate accuracy in the internal validation cohort\n",
    "GS2 = GridSearchCV(GDB_model, parameter,scoring='accuracy', cv=5)\n",
    "model2 = GS2.fit(X,y)\n",
    "result = model2.cv_results_\n",
    "acc_result = np.concatenate([result['split0_test_score'],result['split1_test_score'],\n",
    "               result['split2_test_score'],result['split3_test_score'],\n",
    "               result['split4_test_score']],axis=0)\n",
    "interval2=stats.norm.interval(0.95,np.mean(acc_result),np.std(acc_result))\n",
    "print('Acc',acc_result)\n",
    "print('Mean Acc',np.mean(acc_result))\n",
    "print('95% CI of Acc',interval2)\n",
    "\n",
    "#caculate accuracy in the internal validation cohort\n",
    "GS3 = GridSearchCV(GDB_model, parameter,scoring='f1', cv=5)\n",
    "model3 = GS3.fit(X,y)\n",
    "result = model3.cv_results_\n",
    "f1_result = np.concatenate([result['split0_test_score'],result['split1_test_score'],\n",
    "               result['split2_test_score'],result['split3_test_score'],\n",
    "               result['split4_test_score']],axis=0)\n",
    "interval3=stats.norm.interval(0.95,np.mean(f1_result),np.std(f1_result))\n",
    "print('F1_score',f1_result)\n",
    "print('Mean F1',np.mean(f1_result))\n",
    "print('95% CI of F1',interval3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6424 - accuracy: 0.6872\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6673 - accuracy: 0.6769\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6382 - accuracy: 0.6718\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6361 - accuracy: 0.7128\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6268 - accuracy: 0.6582\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000000020928F28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6361 - accuracy: 0.6821\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000000208D2840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.7360 - accuracy: 0.6103\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6657 - accuracy: 0.6769\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6710 - accuracy: 0.6256\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6468 - accuracy: 0.6327\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6480 - accuracy: 0.6769\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6635 - accuracy: 0.6718\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6579 - accuracy: 0.6308\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6416 - accuracy: 0.6256\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.5981 - accuracy: 0.6888\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6570 - accuracy: 0.6256\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6660 - accuracy: 0.6667\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6655 - accuracy: 0.6769\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.7262 - accuracy: 0.6205\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6370 - accuracy: 0.6888\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6437 - accuracy: 0.6769\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6310 - accuracy: 0.6923\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6400 - accuracy: 0.6923\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6555 - accuracy: 0.6667\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6202 - accuracy: 0.6888\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6437 - accuracy: 0.6205\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6867 - accuracy: 0.6103\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6685 - accuracy: 0.6769\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6431 - accuracy: 0.6205\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6168 - accuracy: 0.6939\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6434 - accuracy: 0.6308\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6368 - accuracy: 0.6923\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6572 - accuracy: 0.6051\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6408 - accuracy: 0.6718\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6147 - accuracy: 0.6786\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6788 - accuracy: 0.6256\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6387 - accuracy: 0.6974\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6690 - accuracy: 0.6769\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6635 - accuracy: 0.6410\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6244 - accuracy: 0.6888\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6450 - accuracy: 0.6769\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6447 - accuracy: 0.7077\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6833 - accuracy: 0.5949\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6445 - accuracy: 0.6718\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6310 - accuracy: 0.6531\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6400 - accuracy: 0.6103\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6305 - accuracy: 0.6872\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6643 - accuracy: 0.5795\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6326 - accuracy: 0.7128\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6326 - accuracy: 0.6480\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6349 - accuracy: 0.6872\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6472 - accuracy: 0.6256\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.7044 - accuracy: 0.5949\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6094 - accuracy: 0.7179\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6176 - accuracy: 0.6480\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6466 - accuracy: 0.6513\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6524 - accuracy: 0.6103\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6538 - accuracy: 0.6769\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.7484 - accuracy: 0.6256\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6593 - accuracy: 0.6480\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6415 - accuracy: 0.6872\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6582 - accuracy: 0.6051\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6593 - accuracy: 0.5949\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6777 - accuracy: 0.6154\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6187 - accuracy: 0.6786\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6211 - accuracy: 0.6872\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6313 - accuracy: 0.6821\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6861 - accuracy: 0.6154\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6210 - accuracy: 0.7077\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6312 - accuracy: 0.6888\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6258 - accuracy: 0.6923\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6510 - accuracy: 0.6667\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6447 - accuracy: 0.6667\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6961 - accuracy: 0.6821\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6354 - accuracy: 0.6122\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6537 - accuracy: 0.6256\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6404 - accuracy: 0.6872\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6807 - accuracy: 0.6205\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6746 - accuracy: 0.6205\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6400 - accuracy: 0.6480\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6593 - accuracy: 0.6564\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6723 - accuracy: 0.6154\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6551 - accuracy: 0.6564\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6394 - accuracy: 0.7128\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6288 - accuracy: 0.6276\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6277 - accuracy: 0.6872\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6592 - accuracy: 0.6051\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6511 - accuracy: 0.6615\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6498 - accuracy: 0.6308\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6364 - accuracy: 0.6786\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6557 - accuracy: 0.6564\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6390 - accuracy: 0.6974\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6525 - accuracy: 0.6718\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6350 - accuracy: 0.6872\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6274 - accuracy: 0.6786\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6496 - accuracy: 0.6513\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6081 - accuracy: 0.6974\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6593 - accuracy: 0.6308\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6372 - accuracy: 0.7128\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6321 - accuracy: 0.6429\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6480 - accuracy: 0.6352\n",
      "{'nb_epoch': 23}\n",
      "0.6523836767346276\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "np.random.seed(2)\n",
    "np.random.seed(2)\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "X1 = X.values\n",
    "y1 = y.values\n",
    "def bliud_model(neuron1=8,neuron2=4,lr=1,momentums=0.7):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(neuron1,activation='relu',input_shape=(24,)))\n",
    "    #model.add(layers.Dropout(dropout))\n",
    "    model.add(layers.Dense(neuron2,activation='relu'))\n",
    "    model.add(layers.Dense(1,activation='sigmoid'))\n",
    "    sgd = SGD(learning_rate=lr,momentum=momentums)\n",
    "    model.compile(optimizer=sgd,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "'''\n",
    " parameters ={'neurons':range(6,20)，'neuron2':[3,4,5,6,7,8,9,10]\n",
    "                 'lr':[0.1,0.2,0.3,0.4,0.5],'momentum':[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "               ,'batch_size':[5,10,15,20,25] ,'nb_epoch':[16,20,24,28,32],\n",
    "               ,'weight_constraint':[1,2,3,4,5],'dropout':[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "                'activation':['softmax','softplus','softsign','relu','tanh','sigmoid','hard_sigmoid','linear']}\n",
    "one layer neuron\n",
    "Beacause of small samples of our dataset and good performance for activation relu,we didn't perform hyperparameter optimization for\n",
    "dropout, weight_constraint, and activation.\n",
    "step1:parameter1 = {'neurons':range(6:20)} best_params_:{'neurons': 15}\n",
    "step2:parameter2 = {'batch_size':range(4,33),'nb_epoch':range(10,30)}  best_params_:{'batch_size': 28, 'nb_epoch': 15}\n",
    "step3:parameter3 = {'lr':[0.001,0.01,0.05,0.1,0.5,1,2.5,5],'momentums':[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]}\n",
    "best_params_: {'lr': 1, 'momentums': 0.8}  best_score_:0.688\n",
    "\n",
    "Two layer neuron\n",
    "Because of small sample size,The maximum number of layers of the neural network is set to 2\n",
    "step1:parameter1 = {'neuron1':range(6,20),'neuron2':range(3,15)} best_params_:{'neuron1': 8, 'neuron2': 4}\n",
    "step2:parameter2 = {'lr':[0.001,0.01,0.05,0.1,0.5,1,2.5,5],'momentums':[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]}\n",
    "best_params_：{'lr': 1, 'momentums': 0.7} best_score_: 0.63029\n",
    "step3:parameter3 = {'batch_size':range(4,65,4)} best_params_:{'batch_size': 32}\n",
    "step4:parameter4 = {'nb_epoch':range(10,30)} best_params_:{'nb_epoch': 22} model.best_score_: 0.657\n",
    "\n",
    "Above all,the MLP_model achieved best performance when the number of layer neuron is set to 1,\n",
    "The optimal parameter is {'neurons': 15},{'batch_size': 28, 'nb_epoch': 15},{'lr': 1, 'momentums': 0.8}\n",
    "'''\n",
    "parameter4 = {'nb_epoch':range(10,30)}\n",
    "MLP_model = KerasClassifier(build_fn=bliud_model,batch_size=32)\n",
    "GS = GridSearchCV(estimator=MLP_model, param_grid=parameter4,scoring='roc_auc', cv=5)\n",
    "model = GS.fit(X,y)\n",
    "\n",
    "print(model.best_params_)\n",
    "print(model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6448 - accuracy: 0.6513\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6671 - accuracy: 0.6769\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6651 - accuracy: 0.6462\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6530 - accuracy: 0.6821\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.6574 - accuracy: 0.6327\n",
      "9/9 [==============================] - 0s 0s/step - loss: 0.6822 - accuracy: 0.6434\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6557 - accuracy: 0.6821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6750 - accuracy: 0.6308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 0s/step - loss: 0.6579 - accuracy: 0.6462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 0s/step - loss: 0.6613 - accuracy: 0.6615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 0s/step - loss: 0.6968 - accuracy: 0.6173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6561 - accuracy: 0.6885\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6479 - accuracy: 0.6923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 0s/step - loss: 0.6803 - accuracy: 0.6256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6790 - accuracy: 0.5897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6291 - accuracy: 0.7179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6808 - accuracy: 0.6224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6556 - accuracy: 0.6803\n",
      "AUC: [0.65510204 0.53787879 0.72972973 0.69912281 0.69327731]\n",
      "Mean AUC: 0.6630221352733516\n",
      "95% CI of AUC (0.5318593227866524, 0.7941849477600509)\n",
      "Acc [0.71428571 0.67346939 0.75510204 0.6122449  0.72916667]\n",
      "Mean Acc 0.6968537414965985\n",
      "95% CI of Acc (0.599066082027547, 0.7946414009656501)\n",
      "F1_score [0.33333333 0.         0.         0.18181818 0.        ]\n",
      "Mean F1 0.10303030303030303\n",
      "95% CI of F1 (-0.16151800304369562, 0.3675786091043017)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "np.random.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "def bliud_model1(neuron1=15,lr=1,momentums=0.8):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(neuron1,activation='relu',input_shape=(24,)))\n",
    "    model.add(layers.Dense(1,activation='sigmoid'))\n",
    "    sgd = SGD(learning_rate=lr,momentum=momentums)\n",
    "    model.compile(optimizer=sgd,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "parameter = {'lr':[1],'momentums':[0.8]}\n",
    "MLP_model = KerasClassifier(build_fn=bliud_model1,batch_size=28,nb_epoch=15)\n",
    "GS1 = GridSearchCV(estimator=MLP_model, param_grid=parameter,scoring='roc_auc', cv=5)\n",
    "model1 = GS1.fit(X,y)\n",
    "result = model1.cv_results_\n",
    "auc_result = np.concatenate([result['split0_test_score'],result['split1_test_score'],\n",
    "               result['split2_test_score'],result['split3_test_score'],\n",
    "               result['split4_test_score']],axis=0)\n",
    "#caculate 95% CI of AUC\n",
    "interval1=stats.norm.interval(0.95,np.mean(auc_result),np.std(auc_result))\n",
    "Tuned_MLP_Model = model1.best_estimator_\n",
    "Tuned_MLP_Model.model1.save('Best_MLP_Model.h5')\n",
    "\n",
    "#caculate accuracy in the internal validation cohort\n",
    "GS2 = GridSearchCV(MLP_model, parameter,scoring='accuracy', cv=5)\n",
    "model2 = GS2.fit(X,y)\n",
    "result = model2.cv_results_\n",
    "acc_result = np.concatenate([result['split0_test_score'],result['split1_test_score'],\n",
    "               result['split2_test_score'],result['split3_test_score'],\n",
    "               result['split4_test_score']],axis=0)\n",
    "interval2=stats.norm.interval(0.95,np.mean(acc_result),np.std(acc_result))\n",
    "\n",
    "\n",
    "#caculate accuracy in the internal validation cohort\n",
    "GS3 = GridSearchCV(MLP_model, parameter,scoring='f1', cv=5)\n",
    "model3 = GS3.fit(X,y)\n",
    "result = model3.cv_results_\n",
    "f1_result = np.concatenate([result['split0_test_score'],result['split1_test_score'],\n",
    "               result['split2_test_score'],result['split3_test_score'],\n",
    "               result['split4_test_score']],axis=0)\n",
    "interval3=stats.norm.interval(0.95,np.mean(f1_result),np.std(f1_result))\n",
    "print('AUC:',auc_result)\n",
    "print('Mean AUC:',np.mean(auc_result))\n",
    "print('95% CI of AUC',interval1)\n",
    "\n",
    "print('Acc',acc_result)\n",
    "print('Mean Acc',np.mean(acc_result))\n",
    "print('95% CI of Acc',interval2)\n",
    "\n",
    "print('F1_score',f1_result)\n",
    "print('Mean F1',np.mean(f1_result))\n",
    "print('95% CI of F1',interval3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##calibration for Random forest\n",
    "import tensorflow as tf\n",
    "np.random.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "def calibration_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(15,activation='relu',input_shape=(24,)))\n",
    "    model.add(layers.Dense(1,activation='sigmoid'))\n",
    "    sgd = SGD(learning_rate=1,momentum=0.8)\n",
    "    model.compile(optimizer=sgd,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "MLP_model = KerasClassifier(calibration_model,batch_size=28,nb_epoch=15)\n",
    "isotonic = CalibratedClassifierCV(MLP_model, cv=5, method='isotonic')\n",
    "#calibrate_MLP = isotonic.fit(X,y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
